---
title: "R Notebook"
output: html_notebook
date: 9 November 2020
---


Downloading csv file from Kaggle Data set - https://www.kaggle.com/austinreese/craigslist-carstrucks-data

```{r}
library(dplyr)
library(rpart)
carData <- read.csv("/Users/rebeccahousey/Documents/GitHub/vehicles.csv", header = TRUE, sep = ',' ,na.strings = "", stringsAsFactors = FALSE)

#carData <- read.csv("~/Documents/GitHub/UsedCarRepo/vehicle#s.csv", header = #TRUE, sep = ',' ,na.strings = "", stringsAsFactors = FALSE)
#str(vehicles)
```
Cleaning data (1)
- removing 'cylinders' in cylinders column 
- removing id, url, region_url, county columns, image_url, description, vin (irrelevant)
- removing NAs 
- changing year to age of car
- converting condition to numerical (0 = salvage, 1 = fair, 2 = good, 3 = excellent, 4 = like new, 5 = new)
- converting dometer to numeric 
```{r} 
carData$cylinders <- gsub('cylinders', "", paste(carData$cylinders))
carData <- select(carData, -c(id, url, region_url, county, image_url, description, vin))
carData <- na.omit(carData)
carData[carData == 'NA'] <- NA
carData <- na.omit(carData)
carData %>% mutate(age = 2020-year) %>% select(-year) -> vehicles
carData <- carData[complete.cases(carData),]

carData$condition[carData$condition == 'salvage'] = 0
carData$condition[carData$condition == 'fair'] = 1
carData$condition[carData$condition == 'good'] = 2
carData$condition[carData$condition == 'excellent'] = 3
carData$condition[carData$condition == 'like new'] = 4
carData$condition[carData$condition == 'new'] = 5
str(carData)
```
making condition, odometer numerical values
making categorical data into factors 
Removing rows w/ models that appear less common 
Removing rows in which the price is 0,1,2 
Removing rows in which the odometer is 0,1,2,3
```{r}
library(data.table)
carData$condition <- as.numeric(as.character(carData$condition))
carData$odometer <- as.numeric(carData$odometer)
categorical <- c("region", "manufacturer", "model", "condition", "cylinders", "fuel", "title_status", "transmission", "drive", "size", "type", "state", "paint_color")

carData[categorical] <- lapply(carData[categorical], factor)
setDT(carData)[, if(.N > 40) .SD, by = model]
carData<- carData %>% group_by(model) %>% filter(n() > 40)


carData <- carData %>% filter(price != 0)
carData <- carData %>% filter(price != 1)
carData <- carData %>% filter(price != 2)
carData <- carData %>% filter(odometer != 0)
carData <- carData %>% filter(odometer != 1)
carData <- carData %>% filter(odometer != 2)

 carData <- carData[carData$price<1111111,]

```
Plots


```{r}
library(ggplot2)
#price & odometer by condition 
g <- ggplot(carData, aes(odometer, price, fill = condition)) + geom_point(position=position_jitter(h=0.1, w=0.1), shape = 21, alpha = .25, size = 2) + ylim(2400,124000)+xlim(0,250000) 
g +scale_fill_discrete(labels = c("salvage", "fair", "good", "excellent", "like new", "new"))
#order by price
carData$
newdata <- carData[order(carData$price),]



b <- ggplot(data = newdata, aes(paint_color, price, fill = paint_color)) +theme(panel.grid.minor=element_blank(),
           panel.grid.major=element_blank())
b+geom_bar(stat="identity")+scale_fill_manual(values=c("#000000", "blue2" , "brown", "white", "green", "grey", "orange", "purple", "red", "slategrey", "white", "yellow")) +ylim(0,1000000) +coord_flip()

```



#price and model
ggplot(data = carData, aes(model, price)) + geom_jitter(color = "blue", alpha = 0.05) + theme_light() + ylim(4000,100000) + coord_flip() + theme(axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=1))
```


Splitting up the data 
Training and Testing Data to use 80% training 20% testing
```{r}
split <- sort(sample(nrow(carData), nrow(carData)*.8))
trainingCar <- carData[split,]
testCar <- carData[-split,]

```

```{r}
library(gbm)          # basic implementation

set.seed(123)
# train GBM model
gbm.fit <- gbm(
  formula = price ~ .-region-model,
  distribution = "gaussian",
  data = trainingCar,
  n.trees = 5000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
)  

# print results
print(gbm.fit)

#Get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")

#relative influence graph
par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit, 
  cBars = 31,
  method = relative.influence,
  las = 2
)
```




```{r}
#Classification Tree
library(rpart)

str(trainingCar)

log <- rpart(price ~ region+ year+manufacturer+model +condition+ cylinders + fuel + odometer+ title_status+ transmission+drive+ size+ type +paint_color+ state+lat+ long, data = trainingCar, method = "class")
```
```{r}
summary(log)

```

```{r}
tree_pred = predict(tree_model, newdata=testCar)
SSE <- sum((tree_pred-testCar$price)^2)
SST <- sum(testCar$price, mean(testCar$price)^2)
R_square <- 1- SSE/SST
RMSE <- sqrt(SSE/nrow(carData))
R_square


#getting mean square error
mape <- mean(abs((real_preds$predicteds - real_preds$actuals))/real_preds$actuals)
mse <- mean((testCar$price-pred_value)^2)
mse
```






